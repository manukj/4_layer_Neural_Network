{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically any a neural network is of 3 steps.\n",
    "- 1:-load the data\n",
    "- 2:-create the module,train it with the data\n",
    "- 3:-test the module and predict the output for new set of input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 :- load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T   \n",
    "test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "train_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 :-create a module and train it \n",
    "- number of layers are 5 i,e 3 hidden layers \n",
    "* 1st layer of size (64) (64) (3)i,e 64 pixels image and RGB(3) varient \n",
    "- layer 2 of unit 20 , layer 3 of unit 7 and layer 4 of unit \n",
    "- The last layer is the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 :- initialize all the parameters needed for each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    # I/P layer dimension O/P parameter \n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    np.random.seed(1)\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 :- calculate the forward propagation \n",
    "- calculate Z using W A_prev and b\n",
    "- using the calculated Z value apply relu until L-1(4) layer to calculate activation function for the Lth layer apply sigmoid function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "     \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev,W,b,activation):\n",
    "    if activation == \"sigmoid\":\n",
    "       \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "    \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    #I/P X and parameters i,e W and b of all the layers O/P cache that has Z and A of all the layer\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 :- calculating cost \n",
    "using cost = -(Ylog(AL)-(1-Y)log(1-AL))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-np.dot(Y,np.log(AL).T)-np.dot(1-Y,np.log(1-AL).T))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.4 :- Backword Propagation \n",
    "to calculate dw db dz of all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA,Z):\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA,Z):\n",
    "     \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,caches,activation):\n",
    "    linear_cache,Z = caches\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,Z)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "    else:\n",
    "        dZ = relu_backward(dA,Z)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation =\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 :- update parameters \n",
    "Once we are done with calculating the gradient the next step is to update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "     \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 :- creating the model by intergrating all the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    #initialize the parameters    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(0,num_iterations):\n",
    "        # forward propagate and find the AL value i,e the last layer activation value\n",
    "        AL,caches = forward_propagation(X,parameters)\n",
    "        #calculate the cost and store in cost array\n",
    "        cost = compute_cost(AL, Y)\n",
    "        #once the cost is calculate the next step is go back and learn the parameters\n",
    "        #i,e calculate the gradient \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # now updating the paramters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # repeating the step for num_iteration times \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niJRSKIiISKn/B0/OSsH3vggjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd59206bb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model_(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3:- Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is 80.0%\n"
     ]
    }
   ],
   "source": [
    "m = test_x.shape[1]\n",
    "n = len(parameters) // 2 # number of layers in the neural network\n",
    "p = np.zeros((1,m))\n",
    "# for pridect the output for test_x dataset    \n",
    "pre, caches = forward_propagation(test_x, parameters)\n",
    "\n",
    "\n",
    "for i in range(0, pre.shape[1]):\n",
    "    if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "    else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "print(\"Accuracy of the model is \"  + str(np.sum((p == test_y)/m)*100)+str(\"%\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
